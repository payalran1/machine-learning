{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining Unstructured Data\n",
    "\n",
    "In this lab, we will experiment with word2vec model. First of all, we need to install a packages named [gensim](https://radimrehurek.com/gensim/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.4.9\n",
      "  latest version: 4.6.7\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/jnyu/anaconda3\n",
      "\n",
      "  added / updated specs: \n",
      "    - nltk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2018.11.29         |           py36_0         146 KB  anaconda\n",
      "    ca-certificates-2019.1.23  |                0         126 KB  anaconda\n",
      "    openssl-1.0.2p             |       h1de35cc_0         3.4 MB  anaconda\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.7 MB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    ca-certificates: 2019.1.23-0       --> 2019.1.23-0       anaconda\n",
      "    certifi:         2018.11.29-py36_0 --> 2018.11.29-py36_0 anaconda\n",
      "    openssl:         1.0.2p-h1de35cc_0 --> 1.0.2p-h1de35cc_0 anaconda\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "certifi 2018.11.29: #################################################### | 100% \n",
      "ca-certificates 2019.1.23: ############################################# | 100% \n",
      "openssl 1.0.2p: ######################################################## | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.4.9\n",
      "  latest version: 4.6.7\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/jnyu/anaconda3\n",
      "\n",
      "  added / updated specs: \n",
      "    - gensim\n",
      "\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    certifi:         2018.11.29-py36_0 anaconda --> 2018.11.29-py36_1000  conda-forge\n",
      "    openssl:         1.0.2p-h1de35cc_0 anaconda --> 1.0.2q-h1de35cc_0     conda-forge\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "    ca-certificates: 2019.1.23-0       anaconda --> 2018.11.29-ha4d7672_0 conda-forge\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "# install packages\n",
    "import sys\n",
    "\n",
    "!conda install --yes --prefix {sys.prefix} -c anaconda nltk \n",
    "!conda install --yes --prefix {sys.prefix} -c conda-forge gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import logging\n",
    "import warnings\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec on synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some synthetic data\n",
    "We manually generate some synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# define training data\n",
    "sentences = [\n",
    "    ['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "    ['this', 'is', 'the', 'second', 'sentence'],\n",
    "    ['yet', 'another', 'sentence'],\n",
    "    ['one', 'more', 'sentence'],\n",
    "    ['and', 'the', 'final', 'sentence']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train word2vec model\n",
    "\n",
    "In [word2vec](https://radimrehurek.com/gensim/models/word2vec.html), there are a few arguments you may wish to configure:\n",
    "\n",
    "- size: (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).\n",
    "- window: (default 5) The maximum distance between a target word and words around the target word.\n",
    "- min_count: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n",
    "- workers: (default 3) The number of threads to use while training.\n",
    "- sg: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-27 16:44:43,303 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-02-27 16:44:43,305 : INFO : collecting all words and their counts\n",
      "2019-02-27 16:44:43,307 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-02-27 16:44:43,309 : INFO : collected 14 word types from a corpus of 22 raw words and 5 sentences\n",
      "2019-02-27 16:44:43,310 : INFO : Loading a fresh vocabulary\n",
      "2019-02-27 16:44:43,312 : INFO : min_count=1 retains 14 unique words (100% of original 14, drops 0)\n",
      "2019-02-27 16:44:43,313 : INFO : min_count=1 leaves 22 word corpus (100% of original 22, drops 0)\n",
      "2019-02-27 16:44:43,315 : INFO : deleting the raw counts dictionary of 14 items\n",
      "2019-02-27 16:44:43,317 : INFO : sample=0.001 downsamples 14 most-common words\n",
      "2019-02-27 16:44:43,319 : INFO : downsampling leaves estimated 2 word corpus (12.7% of prior 22)\n",
      "2019-02-27 16:44:43,323 : INFO : estimated required memory for 14 words and 10 dimensions: 8120 bytes\n",
      "2019-02-27 16:44:43,324 : INFO : resetting layer weights\n",
      "2019-02-27 16:44:43,326 : INFO : training model with 3 workers on 14 vocabulary and 10 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-02-27 16:44:43,336 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-27 16:44:43,338 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-27 16:44:43,339 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-27 16:44:43,341 : INFO : EPOCH - 1 : training on 22 raw words (1 effective words) took 0.0s, 206 effective words/s\n",
      "2019-02-27 16:44:43,344 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-27 16:44:43,345 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-27 16:44:43,347 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-27 16:44:43,348 : INFO : EPOCH - 2 : training on 22 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-02-27 16:44:43,351 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-27 16:44:43,352 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-27 16:44:43,353 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-27 16:44:43,355 : INFO : EPOCH - 3 : training on 22 raw words (4 effective words) took 0.0s, 889 effective words/s\n",
      "2019-02-27 16:44:43,357 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-27 16:44:43,358 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-27 16:44:43,360 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-27 16:44:43,361 : INFO : EPOCH - 4 : training on 22 raw words (3 effective words) took 0.0s, 638 effective words/s\n",
      "2019-02-27 16:44:43,366 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-27 16:44:43,367 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-27 16:44:43,368 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-27 16:44:43,369 : INFO : EPOCH - 5 : training on 22 raw words (2 effective words) took 0.0s, 518 effective words/s\n",
      "2019-02-27 16:44:43,370 : INFO : training on a 110 raw words (10 effective words) took 0.0s, 233 effective words/s\n",
      "2019-02-27 16:44:43,372 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-02-27 16:44:43,374 : INFO : saving Word2Vec object under model.bin, separately None\n",
      "2019-02-27 16:44:43,375 : INFO : not storing attribute vectors_norm\n",
      "2019-02-27 16:44:43,376 : INFO : not storing attribute cum_table\n",
      "2019-02-27 16:44:43,379 : INFO : saved model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14, size=10, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model = Word2Vec(sentences, size=10, min_count=1)\n",
    "\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "\n",
    "# save the model\n",
    "model.save('model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the model\n",
    "\n",
    "After the model is trained, it is accessible via the “wv” attribute. This is the actual word vector model in which queries can be made.\n",
    "\n",
    "For example, you can print the learned vocabulary of tokens (words) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec', 'second', 'yet', 'another', 'one', 'more', 'and', 'final']\n"
     ]
    }
   ],
   "source": [
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, you can access the embedding of a particular word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01425273  0.03096979 -0.02607897 -0.03749254 -0.02945738 -0.00771068\n",
      " -0.01760446  0.01088096 -0.02845169 -0.02757283]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jnyu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# access vector for one word\n",
    "print(model['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-27 16:44:44,157 : INFO : loading Word2Vec object from model.bin\n",
      "2019-02-27 16:44:44,159 : INFO : loading wv recursively from model.bin.wv.* with mmap=None\n",
      "2019-02-27 16:44:44,161 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-02-27 16:44:44,162 : INFO : loading vocabulary recursively from model.bin.vocabulary.* with mmap=None\n",
      "2019-02-27 16:44:44,164 : INFO : loading trainables recursively from model.bin.trainables.* with mmap=None\n",
      "2019-02-27 16:44:44,166 : INFO : setting ignored attribute cum_table to None\n",
      "2019-02-27 16:44:44,169 : INFO : loaded model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14, size=10, alpha=0.025)\n",
      "['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec', 'second', 'yet', 'another', 'one', 'more', 'and', 'final']\n",
      "[-0.01425273  0.03096979 -0.02607897 -0.03749254 -0.02945738 -0.00771068\n",
      " -0.01760446  0.01088096 -0.02845169 -0.02757283]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jnyu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)\n",
    "\n",
    "\n",
    "# summarize vocabulary\n",
    "new_words = list(model.wv.vocab)\n",
    "print(new_words)\n",
    "print(new_model['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec on user review data\n",
    "\n",
    "In this task, we will apply word2vec to a real world data [OpinRank](http://kavita-ganesan.com/entity-ranking-data/) dataset. This dataset has full user reviews of cars and hotels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    " \n",
    "    print(\"reading file {0}...this may take a while\".format(input_file))\n",
    "    with gzip.open(input_file, 'rb') as f:\n",
    "        for i, line in enumerate(f):\n",
    " \n",
    "            if (i % 10000 == 0):\n",
    "                print(\"read {0} reviews\".format(i))\n",
    "            # do some pre-processing and return list of words for each review b text\n",
    "            yield gensim.utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file reviews_data.txt.gz...this may take a while\n",
      "read 0 reviews\n",
      "read 10000 reviews\n",
      "read 20000 reviews\n",
      "read 30000 reviews\n",
      "read 40000 reviews\n",
      "read 50000 reviews\n",
      "read 60000 reviews\n",
      "read 70000 reviews\n",
      "read 80000 reviews\n",
      "read 90000 reviews\n",
      "read 100000 reviews\n",
      "read 110000 reviews\n",
      "read 120000 reviews\n",
      "read 130000 reviews\n",
      "read 140000 reviews\n",
      "read 150000 reviews\n",
      "read 160000 reviews\n",
      "read 170000 reviews\n",
      "read 180000 reviews\n",
      "read 190000 reviews\n",
      "read 200000 reviews\n",
      "read 210000 reviews\n",
      "read 220000 reviews\n",
      "read 230000 reviews\n",
      "read 240000 reviews\n",
      "read 250000 reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-27 17:00:15,413 : INFO : Done reading data file\n"
     ]
    }
   ],
   "source": [
    "documents = list (read_input('reviews_data.txt.gz'))\n",
    "logging.info (\"Done reading data file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-27 17:02:37,316 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-02-27 17:02:37,319 : INFO : collecting all words and their counts\n",
      "2019-02-27 17:02:37,320 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-02-27 17:02:37,539 : INFO : PROGRESS: at sentence #10000, processed 1655714 words, keeping 25777 word types\n",
      "2019-02-27 17:02:37,768 : INFO : PROGRESS: at sentence #20000, processed 3317863 words, keeping 35016 word types\n",
      "2019-02-27 17:02:38,038 : INFO : PROGRESS: at sentence #30000, processed 5264072 words, keeping 47518 word types\n",
      "2019-02-27 17:02:38,307 : INFO : PROGRESS: at sentence #40000, processed 7081746 words, keeping 56675 word types\n",
      "2019-02-27 17:02:38,571 : INFO : PROGRESS: at sentence #50000, processed 9089491 words, keeping 63744 word types\n",
      "2019-02-27 17:02:38,834 : INFO : PROGRESS: at sentence #60000, processed 11013723 words, keeping 76781 word types\n",
      "2019-02-27 17:02:39,048 : INFO : PROGRESS: at sentence #70000, processed 12637525 words, keeping 83194 word types\n",
      "2019-02-27 17:02:39,283 : INFO : PROGRESS: at sentence #80000, processed 14099751 words, keeping 88454 word types\n",
      "2019-02-27 17:02:39,500 : INFO : PROGRESS: at sentence #90000, processed 15662149 words, keeping 93352 word types\n",
      "2019-02-27 17:02:39,758 : INFO : PROGRESS: at sentence #100000, processed 17164487 words, keeping 97881 word types\n",
      "2019-02-27 17:02:40,013 : INFO : PROGRESS: at sentence #110000, processed 18652292 words, keeping 102127 word types\n",
      "2019-02-27 17:02:40,270 : INFO : PROGRESS: at sentence #120000, processed 20152529 words, keeping 105918 word types\n",
      "2019-02-27 17:02:40,513 : INFO : PROGRESS: at sentence #130000, processed 21684330 words, keeping 110099 word types\n",
      "2019-02-27 17:02:40,782 : INFO : PROGRESS: at sentence #140000, processed 23330206 words, keeping 114103 word types\n",
      "2019-02-27 17:02:41,031 : INFO : PROGRESS: at sentence #150000, processed 24838754 words, keeping 118169 word types\n",
      "2019-02-27 17:02:41,275 : INFO : PROGRESS: at sentence #160000, processed 26390910 words, keeping 118665 word types\n",
      "2019-02-27 17:02:41,511 : INFO : PROGRESS: at sentence #170000, processed 27913916 words, keeping 123350 word types\n",
      "2019-02-27 17:02:41,763 : INFO : PROGRESS: at sentence #180000, processed 29535612 words, keeping 126742 word types\n",
      "2019-02-27 17:02:42,024 : INFO : PROGRESS: at sentence #190000, processed 31096459 words, keeping 129841 word types\n",
      "2019-02-27 17:02:42,288 : INFO : PROGRESS: at sentence #200000, processed 32805271 words, keeping 133249 word types\n",
      "2019-02-27 17:02:42,533 : INFO : PROGRESS: at sentence #210000, processed 34434198 words, keeping 136358 word types\n",
      "2019-02-27 17:02:42,760 : INFO : PROGRESS: at sentence #220000, processed 36083482 words, keeping 139412 word types\n",
      "2019-02-27 17:02:42,987 : INFO : PROGRESS: at sentence #230000, processed 37571762 words, keeping 142393 word types\n",
      "2019-02-27 17:02:43,199 : INFO : PROGRESS: at sentence #240000, processed 39138190 words, keeping 145226 word types\n",
      "2019-02-27 17:02:43,415 : INFO : PROGRESS: at sentence #250000, processed 40695049 words, keeping 147960 word types\n",
      "2019-02-27 17:02:43,535 : INFO : collected 150053 word types from a corpus of 41519355 raw words and 255404 sentences\n",
      "2019-02-27 17:02:43,537 : INFO : Loading a fresh vocabulary\n",
      "2019-02-27 17:02:44,458 : INFO : min_count=2 retains 70538 unique words (47% of original 150053, drops 79515)\n",
      "2019-02-27 17:02:44,459 : INFO : min_count=2 leaves 41439840 word corpus (99% of original 41519355, drops 79515)\n",
      "2019-02-27 17:02:44,633 : INFO : deleting the raw counts dictionary of 150053 items\n",
      "2019-02-27 17:02:44,637 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2019-02-27 17:02:44,639 : INFO : downsampling leaves estimated 30349255 word corpus (73.2% of prior 41439840)\n",
      "2019-02-27 17:02:44,850 : INFO : estimated required memory for 70538 words and 150 dimensions: 119914600 bytes\n",
      "2019-02-27 17:02:44,852 : INFO : resetting layer weights\n",
      "2019-02-27 17:02:45,554 : INFO : training model with 10 workers on 70538 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-02-27 17:02:46,564 : INFO : EPOCH 1 - PROGRESS: at 3.26% examples, 1004215 words/s, in_qsize 17, out_qsize 2\n",
      "2019-02-27 17:02:47,567 : INFO : EPOCH 1 - PROGRESS: at 7.03% examples, 1076748 words/s, in_qsize 20, out_qsize 0\n",
      "2019-02-27 17:02:48,577 : INFO : EPOCH 1 - PROGRESS: at 10.41% examples, 1102498 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:02:49,581 : INFO : EPOCH 1 - PROGRESS: at 13.56% examples, 1109593 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:02:50,591 : INFO : EPOCH 1 - PROGRESS: at 16.89% examples, 1119083 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:02:51,595 : INFO : EPOCH 1 - PROGRESS: at 19.93% examples, 1123188 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:02:52,596 : INFO : EPOCH 1 - PROGRESS: at 23.23% examples, 1129459 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:02:53,601 : INFO : EPOCH 1 - PROGRESS: at 26.87% examples, 1130747 words/s, in_qsize 20, out_qsize 1\n",
      "2019-02-27 17:02:54,610 : INFO : EPOCH 1 - PROGRESS: at 31.08% examples, 1132372 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:02:55,611 : INFO : EPOCH 1 - PROGRESS: at 35.02% examples, 1133060 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:02:56,614 : INFO : EPOCH 1 - PROGRESS: at 38.89% examples, 1128640 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:02:57,636 : INFO : EPOCH 1 - PROGRESS: at 43.12% examples, 1130324 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:02:58,651 : INFO : EPOCH 1 - PROGRESS: at 47.11% examples, 1128066 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:02:59,661 : INFO : EPOCH 1 - PROGRESS: at 50.98% examples, 1125414 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:00,670 : INFO : EPOCH 1 - PROGRESS: at 54.71% examples, 1126086 words/s, in_qsize 20, out_qsize 0\n",
      "2019-02-27 17:03:01,673 : INFO : EPOCH 1 - PROGRESS: at 58.55% examples, 1123647 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:02,673 : INFO : EPOCH 1 - PROGRESS: at 62.42% examples, 1123562 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:03:03,681 : INFO : EPOCH 1 - PROGRESS: at 66.33% examples, 1122234 words/s, in_qsize 18, out_qsize 4\n",
      "2019-02-27 17:03:04,682 : INFO : EPOCH 1 - PROGRESS: at 70.05% examples, 1122466 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:03:05,699 : INFO : EPOCH 1 - PROGRESS: at 73.97% examples, 1122050 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:06,714 : INFO : EPOCH 1 - PROGRESS: at 77.44% examples, 1120640 words/s, in_qsize 18, out_qsize 2\n",
      "2019-02-27 17:03:07,716 : INFO : EPOCH 1 - PROGRESS: at 81.13% examples, 1121917 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:03:08,728 : INFO : EPOCH 1 - PROGRESS: at 84.85% examples, 1121639 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:09,736 : INFO : EPOCH 1 - PROGRESS: at 89.04% examples, 1123774 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:10,741 : INFO : EPOCH 1 - PROGRESS: at 92.81% examples, 1122509 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:03:11,746 : INFO : EPOCH 1 - PROGRESS: at 96.64% examples, 1122146 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:12,530 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-27 17:03:12,534 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-27 17:03:12,536 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-27 17:03:12,538 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-27 17:03:12,564 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-27 17:03:12,566 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-27 17:03:12,570 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-27 17:03:12,577 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-27 17:03:12,581 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-27 17:03:12,583 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-27 17:03:12,584 : INFO : EPOCH - 1 : training on 41519355 raw words (30348660 effective words) took 27.0s, 1122949 effective words/s\n",
      "2019-02-27 17:03:13,624 : INFO : EPOCH 2 - PROGRESS: at 3.43% examples, 1038304 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-27 17:03:14,628 : INFO : EPOCH 2 - PROGRESS: at 7.17% examples, 1085144 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:15,632 : INFO : EPOCH 2 - PROGRESS: at 10.39% examples, 1093301 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:16,634 : INFO : EPOCH 2 - PROGRESS: at 13.54% examples, 1103319 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:17,637 : INFO : EPOCH 2 - PROGRESS: at 16.84% examples, 1112678 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:18,641 : INFO : EPOCH 2 - PROGRESS: at 19.81% examples, 1114361 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:19,643 : INFO : EPOCH 2 - PROGRESS: at 23.02% examples, 1115730 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:20,647 : INFO : EPOCH 2 - PROGRESS: at 26.50% examples, 1115946 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:21,655 : INFO : EPOCH 2 - PROGRESS: at 30.53% examples, 1116032 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:03:22,667 : INFO : EPOCH 2 - PROGRESS: at 34.55% examples, 1116433 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:03:23,673 : INFO : EPOCH 2 - PROGRESS: at 38.58% examples, 1118420 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:24,675 : INFO : EPOCH 2 - PROGRESS: at 42.74% examples, 1121696 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:03:25,681 : INFO : EPOCH 2 - PROGRESS: at 46.73% examples, 1119805 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:26,686 : INFO : EPOCH 2 - PROGRESS: at 50.70% examples, 1120646 words/s, in_qsize 17, out_qsize 2\n",
      "2019-02-27 17:03:27,700 : INFO : EPOCH 2 - PROGRESS: at 54.39% examples, 1120273 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:28,706 : INFO : EPOCH 2 - PROGRESS: at 58.54% examples, 1123858 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:29,707 : INFO : EPOCH 2 - PROGRESS: at 62.46% examples, 1124187 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:30,712 : INFO : EPOCH 2 - PROGRESS: at 66.33% examples, 1122556 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:03:31,725 : INFO : EPOCH 2 - PROGRESS: at 70.04% examples, 1122020 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:32,727 : INFO : EPOCH 2 - PROGRESS: at 73.99% examples, 1122820 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:33,727 : INFO : EPOCH 2 - PROGRESS: at 77.50% examples, 1122829 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:34,733 : INFO : EPOCH 2 - PROGRESS: at 81.20% examples, 1123476 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:03:35,749 : INFO : EPOCH 2 - PROGRESS: at 84.93% examples, 1123570 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:36,759 : INFO : EPOCH 2 - PROGRESS: at 88.95% examples, 1123206 words/s, in_qsize 17, out_qsize 2\n",
      "2019-02-27 17:03:37,770 : INFO : EPOCH 2 - PROGRESS: at 92.88% examples, 1123666 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:03:38,770 : INFO : EPOCH 2 - PROGRESS: at 96.79% examples, 1124024 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:39,548 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-27 17:03:39,555 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-27 17:03:39,556 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-27 17:03:39,562 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-27 17:03:39,566 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-27 17:03:39,571 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-27 17:03:39,576 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-27 17:03:39,578 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-27 17:03:39,586 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-27 17:03:39,588 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-27 17:03:39,590 : INFO : EPOCH - 2 : training on 41519355 raw words (30347052 effective words) took 27.0s, 1124174 effective words/s\n",
      "2019-02-27 17:03:40,598 : INFO : EPOCH 3 - PROGRESS: at 3.50% examples, 1087752 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:41,601 : INFO : EPOCH 3 - PROGRESS: at 7.23% examples, 1114186 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:03:42,603 : INFO : EPOCH 3 - PROGRESS: at 10.25% examples, 1089367 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:43,608 : INFO : EPOCH 3 - PROGRESS: at 13.44% examples, 1101621 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:03:44,611 : INFO : EPOCH 3 - PROGRESS: at 16.56% examples, 1099596 words/s, in_qsize 20, out_qsize 2\n",
      "2019-02-27 17:03:45,624 : INFO : EPOCH 3 - PROGRESS: at 19.61% examples, 1104295 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:46,629 : INFO : EPOCH 3 - PROGRESS: at 22.84% examples, 1106462 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:47,639 : INFO : EPOCH 3 - PROGRESS: at 26.26% examples, 1109805 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:48,639 : INFO : EPOCH 3 - PROGRESS: at 30.34% examples, 1112261 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:49,644 : INFO : EPOCH 3 - PROGRESS: at 34.38% examples, 1113857 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:50,647 : INFO : EPOCH 3 - PROGRESS: at 38.19% examples, 1111167 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:51,654 : INFO : EPOCH 3 - PROGRESS: at 42.28% examples, 1111655 words/s, in_qsize 17, out_qsize 2\n",
      "2019-02-27 17:03:52,655 : INFO : EPOCH 3 - PROGRESS: at 46.38% examples, 1113687 words/s, in_qsize 19, out_qsize 3\n",
      "2019-02-27 17:03:53,658 : INFO : EPOCH 3 - PROGRESS: at 50.31% examples, 1115085 words/s, in_qsize 20, out_qsize 2\n",
      "2019-02-27 17:03:54,661 : INFO : EPOCH 3 - PROGRESS: at 54.08% examples, 1116793 words/s, in_qsize 17, out_qsize 2\n",
      "2019-02-27 17:03:55,677 : INFO : EPOCH 3 - PROGRESS: at 58.01% examples, 1115851 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:03:56,678 : INFO : EPOCH 3 - PROGRESS: at 61.91% examples, 1117053 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:03:57,695 : INFO : EPOCH 3 - PROGRESS: at 65.98% examples, 1117963 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:58,697 : INFO : EPOCH 3 - PROGRESS: at 69.61% examples, 1116096 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:03:59,703 : INFO : EPOCH 3 - PROGRESS: at 73.48% examples, 1116900 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:00,704 : INFO : EPOCH 3 - PROGRESS: at 77.06% examples, 1117140 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:01,708 : INFO : EPOCH 3 - PROGRESS: at 80.62% examples, 1116521 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:02,725 : INFO : EPOCH 3 - PROGRESS: at 84.35% examples, 1116348 words/s, in_qsize 17, out_qsize 2\n",
      "2019-02-27 17:04:03,736 : INFO : EPOCH 3 - PROGRESS: at 88.42% examples, 1118535 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:04:04,745 : INFO : EPOCH 3 - PROGRESS: at 92.14% examples, 1115296 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:05,754 : INFO : EPOCH 3 - PROGRESS: at 95.84% examples, 1114273 words/s, in_qsize 17, out_qsize 2\n",
      "2019-02-27 17:04:06,736 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-27 17:04:06,741 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-27 17:04:06,744 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-27 17:04:06,764 : INFO : EPOCH 3 - PROGRESS: at 99.87% examples, 1115499 words/s, in_qsize 6, out_qsize 1\n",
      "2019-02-27 17:04:06,766 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-27 17:04:06,777 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-27 17:04:06,785 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-27 17:04:06,796 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-27 17:04:06,798 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-27 17:04:06,800 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-27 17:04:06,806 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-27 17:04:06,807 : INFO : EPOCH - 3 : training on 41519355 raw words (30346034 effective words) took 27.2s, 1115223 effective words/s\n",
      "2019-02-27 17:04:07,816 : INFO : EPOCH 4 - PROGRESS: at 3.57% examples, 1106056 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:08,819 : INFO : EPOCH 4 - PROGRESS: at 7.25% examples, 1116981 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-27 17:04:09,820 : INFO : EPOCH 4 - PROGRESS: at 10.57% examples, 1122936 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:10,835 : INFO : EPOCH 4 - PROGRESS: at 13.69% examples, 1119892 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:11,843 : INFO : EPOCH 4 - PROGRESS: at 16.95% examples, 1123218 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:12,850 : INFO : EPOCH 4 - PROGRESS: at 19.98% examples, 1126219 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:13,871 : INFO : EPOCH 4 - PROGRESS: at 23.18% examples, 1122928 words/s, in_qsize 17, out_qsize 2\n",
      "2019-02-27 17:04:14,874 : INFO : EPOCH 4 - PROGRESS: at 26.20% examples, 1105597 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:15,881 : INFO : EPOCH 4 - PROGRESS: at 29.67% examples, 1087807 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:16,886 : INFO : EPOCH 4 - PROGRESS: at 33.38% examples, 1080967 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:17,892 : INFO : EPOCH 4 - PROGRESS: at 36.84% examples, 1074160 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:18,906 : INFO : EPOCH 4 - PROGRESS: at 40.39% examples, 1066819 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:19,914 : INFO : EPOCH 4 - PROGRESS: at 44.01% examples, 1060103 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:20,920 : INFO : EPOCH 4 - PROGRESS: at 47.37% examples, 1052432 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:21,923 : INFO : EPOCH 4 - PROGRESS: at 50.94% examples, 1049811 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:22,934 : INFO : EPOCH 4 - PROGRESS: at 54.20% examples, 1045991 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:23,939 : INFO : EPOCH 4 - PROGRESS: at 57.44% examples, 1037833 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:24,939 : INFO : EPOCH 4 - PROGRESS: at 60.58% examples, 1031271 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:25,955 : INFO : EPOCH 4 - PROGRESS: at 63.85% examples, 1024110 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:26,965 : INFO : EPOCH 4 - PROGRESS: at 66.97% examples, 1018688 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:27,968 : INFO : EPOCH 4 - PROGRESS: at 70.04% examples, 1014629 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:28,985 : INFO : EPOCH 4 - PROGRESS: at 73.15% examples, 1009399 words/s, in_qsize 20, out_qsize 2\n",
      "2019-02-27 17:04:29,988 : INFO : EPOCH 4 - PROGRESS: at 76.22% examples, 1006145 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:30,991 : INFO : EPOCH 4 - PROGRESS: at 79.16% examples, 1002450 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:32,005 : INFO : EPOCH 4 - PROGRESS: at 82.34% examples, 1000442 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:33,019 : INFO : EPOCH 4 - PROGRESS: at 85.38% examples, 997947 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:34,024 : INFO : EPOCH 4 - PROGRESS: at 88.84% examples, 996350 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:35,027 : INFO : EPOCH 4 - PROGRESS: at 92.07% examples, 993691 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:04:36,030 : INFO : EPOCH 4 - PROGRESS: at 95.19% examples, 991188 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:37,058 : INFO : EPOCH 4 - PROGRESS: at 98.55% examples, 989905 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:37,437 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-27 17:04:37,440 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-27 17:04:37,457 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-27 17:04:37,466 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-27 17:04:37,468 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-27 17:04:37,484 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-27 17:04:37,488 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-27 17:04:37,492 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-27 17:04:37,494 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-27 17:04:37,501 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-27 17:04:37,503 : INFO : EPOCH - 4 : training on 41519355 raw words (30348763 effective words) took 30.7s, 988853 effective words/s\n",
      "2019-02-27 17:04:38,512 : INFO : EPOCH 5 - PROGRESS: at 2.89% examples, 900048 words/s, in_qsize 16, out_qsize 3\n",
      "2019-02-27 17:04:39,525 : INFO : EPOCH 5 - PROGRESS: at 6.64% examples, 1015508 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:40,532 : INFO : EPOCH 5 - PROGRESS: at 9.93% examples, 1047826 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:41,534 : INFO : EPOCH 5 - PROGRESS: at 13.10% examples, 1071444 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:42,540 : INFO : EPOCH 5 - PROGRESS: at 16.35% examples, 1080726 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:43,543 : INFO : EPOCH 5 - PROGRESS: at 19.31% examples, 1084297 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:44,559 : INFO : EPOCH 5 - PROGRESS: at 22.58% examples, 1090730 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:45,561 : INFO : EPOCH 5 - PROGRESS: at 25.83% examples, 1094368 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:46,564 : INFO : EPOCH 5 - PROGRESS: at 29.96% examples, 1098176 words/s, in_qsize 16, out_qsize 3\n",
      "2019-02-27 17:04:47,571 : INFO : EPOCH 5 - PROGRESS: at 34.00% examples, 1102417 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:04:48,572 : INFO : EPOCH 5 - PROGRESS: at 37.94% examples, 1103721 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:49,575 : INFO : EPOCH 5 - PROGRESS: at 41.96% examples, 1102848 words/s, in_qsize 17, out_qsize 2\n",
      "2019-02-27 17:04:50,578 : INFO : EPOCH 5 - PROGRESS: at 46.07% examples, 1105353 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:51,578 : INFO : EPOCH 5 - PROGRESS: at 50.00% examples, 1108139 words/s, in_qsize 19, out_qsize 1\n",
      "2019-02-27 17:04:52,581 : INFO : EPOCH 5 - PROGRESS: at 53.76% examples, 1110302 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:53,586 : INFO : EPOCH 5 - PROGRESS: at 57.71% examples, 1111030 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:54,589 : INFO : EPOCH 5 - PROGRESS: at 61.52% examples, 1110631 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:55,595 : INFO : EPOCH 5 - PROGRESS: at 65.21% examples, 1105086 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:56,600 : INFO : EPOCH 5 - PROGRESS: at 69.09% examples, 1107889 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:04:57,606 : INFO : EPOCH 5 - PROGRESS: at 72.94% examples, 1110513 words/s, in_qsize 20, out_qsize 1\n",
      "2019-02-27 17:04:58,611 : INFO : EPOCH 5 - PROGRESS: at 76.68% examples, 1111862 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:04:59,622 : INFO : EPOCH 5 - PROGRESS: at 80.27% examples, 1111810 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:00,631 : INFO : EPOCH 5 - PROGRESS: at 83.97% examples, 1111555 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:01,634 : INFO : EPOCH 5 - PROGRESS: at 87.93% examples, 1113399 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:05:02,648 : INFO : EPOCH 5 - PROGRESS: at 91.89% examples, 1113284 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:03,664 : INFO : EPOCH 5 - PROGRESS: at 95.78% examples, 1113686 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:05:04,666 : INFO : EPOCH 5 - PROGRESS: at 99.78% examples, 1115271 words/s, in_qsize 9, out_qsize 1\n",
      "2019-02-27 17:05:04,668 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-27 17:05:04,684 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-27 17:05:04,685 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-27 17:05:04,696 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-27 17:05:04,716 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-27 17:05:04,724 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-27 17:05:04,732 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-27 17:05:04,737 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-27 17:05:04,739 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-27 17:05:04,740 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-27 17:05:04,741 : INFO : EPOCH - 5 : training on 41519355 raw words (30347365 effective words) took 27.2s, 1114434 effective words/s\n",
      "2019-02-27 17:05:04,743 : INFO : training on a 207596775 raw words (151737874 effective words) took 139.2s, 1090179 effective words/s\n",
      "2019-02-27 17:05:04,745 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-02-27 17:05:04,746 : INFO : training model with 10 workers on 70538 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-02-27 17:05:05,761 : INFO : EPOCH 1 - PROGRESS: at 3.36% examples, 1035836 words/s, in_qsize 17, out_qsize 2\n",
      "2019-02-27 17:05:06,762 : INFO : EPOCH 1 - PROGRESS: at 7.13% examples, 1093665 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:05:07,763 : INFO : EPOCH 1 - PROGRESS: at 10.55% examples, 1121821 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:05:08,765 : INFO : EPOCH 1 - PROGRESS: at 13.63% examples, 1119332 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:09,770 : INFO : EPOCH 1 - PROGRESS: at 16.81% examples, 1116176 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:10,786 : INFO : EPOCH 1 - PROGRESS: at 19.78% examples, 1115236 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:11,797 : INFO : EPOCH 1 - PROGRESS: at 23.10% examples, 1121111 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:12,802 : INFO : EPOCH 1 - PROGRESS: at 26.67% examples, 1122577 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:05:13,803 : INFO : EPOCH 1 - PROGRESS: at 30.62% examples, 1119578 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:14,807 : INFO : EPOCH 1 - PROGRESS: at 34.53% examples, 1117619 words/s, in_qsize 17, out_qsize 2\n",
      "2019-02-27 17:05:15,809 : INFO : EPOCH 1 - PROGRESS: at 38.50% examples, 1118592 words/s, in_qsize 20, out_qsize 0\n",
      "2019-02-27 17:05:16,819 : INFO : EPOCH 1 - PROGRESS: at 42.69% examples, 1121709 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:17,824 : INFO : EPOCH 1 - PROGRESS: at 46.74% examples, 1120966 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:18,824 : INFO : EPOCH 1 - PROGRESS: at 50.63% examples, 1120641 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:19,835 : INFO : EPOCH 1 - PROGRESS: at 54.24% examples, 1118989 words/s, in_qsize 17, out_qsize 2\n",
      "2019-02-27 17:05:20,838 : INFO : EPOCH 1 - PROGRESS: at 58.32% examples, 1121575 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:21,838 : INFO : EPOCH 1 - PROGRESS: at 62.18% examples, 1121589 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:22,845 : INFO : EPOCH 1 - PROGRESS: at 66.19% examples, 1121709 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:23,866 : INFO : EPOCH 1 - PROGRESS: at 69.30% examples, 1110227 words/s, in_qsize 16, out_qsize 3\n",
      "2019-02-27 17:05:24,878 : INFO : EPOCH 1 - PROGRESS: at 73.19% examples, 1112424 words/s, in_qsize 20, out_qsize 0\n",
      "2019-02-27 17:05:25,888 : INFO : EPOCH 1 - PROGRESS: at 76.90% examples, 1113147 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:26,896 : INFO : EPOCH 1 - PROGRESS: at 80.61% examples, 1115094 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:27,898 : INFO : EPOCH 1 - PROGRESS: at 84.25% examples, 1114135 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:28,909 : INFO : EPOCH 1 - PROGRESS: at 88.25% examples, 1115714 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:29,918 : INFO : EPOCH 1 - PROGRESS: at 92.22% examples, 1115501 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:30,935 : INFO : EPOCH 1 - PROGRESS: at 96.11% examples, 1116573 words/s, in_qsize 16, out_qsize 3\n",
      "2019-02-27 17:05:31,852 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-27 17:05:31,854 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-27 17:05:31,865 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-27 17:05:31,880 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-27 17:05:31,893 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-27 17:05:31,899 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-27 17:05:31,905 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-27 17:05:31,910 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-27 17:05:31,919 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-27 17:05:31,921 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-27 17:05:31,922 : INFO : EPOCH - 1 : training on 41519355 raw words (30348853 effective words) took 27.2s, 1116958 effective words/s\n",
      "2019-02-27 17:05:32,932 : INFO : EPOCH 2 - PROGRESS: at 3.51% examples, 1084321 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:33,939 : INFO : EPOCH 2 - PROGRESS: at 7.18% examples, 1100716 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:34,939 : INFO : EPOCH 2 - PROGRESS: at 10.44% examples, 1107502 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:35,942 : INFO : EPOCH 2 - PROGRESS: at 13.59% examples, 1115452 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:36,951 : INFO : EPOCH 2 - PROGRESS: at 16.72% examples, 1109514 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:37,960 : INFO : EPOCH 2 - PROGRESS: at 19.74% examples, 1112113 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:38,985 : INFO : EPOCH 2 - PROGRESS: at 22.88% examples, 1104989 words/s, in_qsize 19, out_qsize 1\n",
      "2019-02-27 17:05:39,988 : INFO : EPOCH 2 - PROGRESS: at 26.39% examples, 1110396 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:40,998 : INFO : EPOCH 2 - PROGRESS: at 30.48% examples, 1113200 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:42,000 : INFO : EPOCH 2 - PROGRESS: at 34.49% examples, 1114307 words/s, in_qsize 20, out_qsize 2\n",
      "2019-02-27 17:05:43,010 : INFO : EPOCH 2 - PROGRESS: at 38.26% examples, 1110324 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:44,032 : INFO : EPOCH 2 - PROGRESS: at 42.31% examples, 1108215 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:45,032 : INFO : EPOCH 2 - PROGRESS: at 45.92% examples, 1099606 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:46,036 : INFO : EPOCH 2 - PROGRESS: at 49.91% examples, 1103546 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:47,050 : INFO : EPOCH 2 - PROGRESS: at 53.52% examples, 1102363 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:48,050 : INFO : EPOCH 2 - PROGRESS: at 57.26% examples, 1099500 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:49,063 : INFO : EPOCH 2 - PROGRESS: at 61.10% examples, 1099584 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:50,070 : INFO : EPOCH 2 - PROGRESS: at 65.19% examples, 1101451 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:51,070 : INFO : EPOCH 2 - PROGRESS: at 69.02% examples, 1103910 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:52,073 : INFO : EPOCH 2 - PROGRESS: at 72.76% examples, 1105468 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:05:53,076 : INFO : EPOCH 2 - PROGRESS: at 76.32% examples, 1104427 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:54,077 : INFO : EPOCH 2 - PROGRESS: at 79.93% examples, 1105208 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:55,081 : INFO : EPOCH 2 - PROGRESS: at 83.81% examples, 1107995 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:56,082 : INFO : EPOCH 2 - PROGRESS: at 87.46% examples, 1107056 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:57,082 : INFO : EPOCH 2 - PROGRESS: at 91.41% examples, 1107262 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:05:58,089 : INFO : EPOCH 2 - PROGRESS: at 95.25% examples, 1107650 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:05:59,091 : INFO : EPOCH 2 - PROGRESS: at 99.18% examples, 1108753 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:05:59,233 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-27 17:05:59,238 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-27 17:05:59,241 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-27 17:05:59,243 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-27 17:05:59,257 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-27 17:05:59,264 : INFO : worker thread finished; awaiting finish of 4 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-27 17:05:59,266 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-27 17:05:59,281 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-27 17:05:59,286 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-27 17:05:59,294 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-27 17:05:59,295 : INFO : EPOCH - 2 : training on 41519355 raw words (30350932 effective words) took 27.4s, 1109014 effective words/s\n",
      "2019-02-27 17:06:00,311 : INFO : EPOCH 3 - PROGRESS: at 3.54% examples, 1085425 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:06:01,317 : INFO : EPOCH 3 - PROGRESS: at 7.28% examples, 1112167 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:06:02,321 : INFO : EPOCH 3 - PROGRESS: at 10.53% examples, 1116491 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:06:03,337 : INFO : EPOCH 3 - PROGRESS: at 13.58% examples, 1107678 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:06:04,343 : INFO : EPOCH 3 - PROGRESS: at 16.93% examples, 1119642 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:06:05,347 : INFO : EPOCH 3 - PROGRESS: at 19.88% examples, 1117742 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:06:06,369 : INFO : EPOCH 3 - PROGRESS: at 23.13% examples, 1118406 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:06:07,380 : INFO : EPOCH 3 - PROGRESS: at 26.66% examples, 1118602 words/s, in_qsize 20, out_qsize 3\n",
      "2019-02-27 17:06:08,389 : INFO : EPOCH 3 - PROGRESS: at 30.88% examples, 1122131 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:06:09,397 : INFO : EPOCH 3 - PROGRESS: at 34.66% examples, 1117348 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:06:10,407 : INFO : EPOCH 3 - PROGRESS: at 38.72% examples, 1118871 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:06:11,410 : INFO : EPOCH 3 - PROGRESS: at 42.78% examples, 1120230 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:06:12,410 : INFO : EPOCH 3 - PROGRESS: at 46.85% examples, 1120532 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:06:13,422 : INFO : EPOCH 3 - PROGRESS: at 50.61% examples, 1116878 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:06:14,428 : INFO : EPOCH 3 - PROGRESS: at 54.24% examples, 1115819 words/s, in_qsize 14, out_qsize 5\n",
      "2019-02-27 17:06:15,445 : INFO : EPOCH 3 - PROGRESS: at 58.30% examples, 1117206 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:06:16,470 : INFO : EPOCH 3 - PROGRESS: at 62.28% examples, 1118057 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:06:17,471 : INFO : EPOCH 3 - PROGRESS: at 66.04% examples, 1114713 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:06:18,481 : INFO : EPOCH 3 - PROGRESS: at 69.87% examples, 1115940 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:06:19,486 : INFO : EPOCH 3 - PROGRESS: at 73.74% examples, 1116437 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:06:20,490 : INFO : EPOCH 3 - PROGRESS: at 77.35% examples, 1117536 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:06:21,495 : INFO : EPOCH 3 - PROGRESS: at 81.00% examples, 1118124 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:06:22,496 : INFO : EPOCH 3 - PROGRESS: at 84.61% examples, 1117371 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:06:23,508 : INFO : EPOCH 3 - PROGRESS: at 88.57% examples, 1117096 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:06:24,516 : INFO : EPOCH 3 - PROGRESS: at 92.58% examples, 1117945 words/s, in_qsize 18, out_qsize 1\n",
      "2019-02-27 17:06:25,517 : INFO : EPOCH 3 - PROGRESS: at 96.38% examples, 1117955 words/s, in_qsize 19, out_qsize 0\n",
      "2019-02-27 17:06:26,399 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-27 17:06:26,401 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-27 17:06:26,406 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-27 17:06:26,415 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-27 17:06:26,419 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-27 17:06:26,424 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-27 17:06:26,426 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-27 17:06:26,428 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-27 17:06:26,430 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-27 17:06:26,435 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-27 17:06:26,437 : INFO : EPOCH - 3 : training on 41519355 raw words (30349427 effective words) took 27.1s, 1118412 effective words/s\n",
      "2019-02-27 17:06:26,438 : INFO : training on a 124558065 raw words (91049212 effective words) took 81.7s, 1114574 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(91049212, 124558065)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build vocabulary and train model\n",
    "model = gensim.models.Word2Vec(\n",
    "    documents,\n",
    "    size=150,\n",
    "    window=10,\n",
    "    min_count=2,\n",
    "    workers=10)\n",
    "model.train(documents, total_examples=len(documents), epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the similarity of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ae31b149d8f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"dirty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "w1 = \"dirty\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('courteous', 0.910372257232666),\n",
       " ('friendly', 0.8230658769607544),\n",
       " ('curteous', 0.818327009677887),\n",
       " ('cordial', 0.8152638077735901),\n",
       " ('freindly', 0.7962666749954224),\n",
       " ('curtious', 0.7900578379631042)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "w1 = [\"polite\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('germany', 0.7337640523910522),\n",
       " ('canada', 0.68161940574646),\n",
       " ('england', 0.6617143750190735),\n",
       " ('hawaii', 0.6576465368270874),\n",
       " ('rome', 0.6501343250274658),\n",
       " ('mexico', 0.6385927200317383)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'france'\n",
    "w1 = [\"france\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the similarity between two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7498892610339847"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two different words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"smelly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two identical words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"dirty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24801825242792697"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two unrelated words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# End of lab 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
